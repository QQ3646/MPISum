# MPI Array Summation

Проект реализует параллельное суммирование элементов массива с использованием **OpenMPI**. Решение представлено на двух языках: **C++** и **Python**.

Вместо стандартной операции `MPI_Reduce`, сборка финальной суммы реализована вручную через парные коммуникации (`Send`/`Recv`) по каскадной схеме (tree-based reduction).

## Алгоритм
Программа реализует паттерн **Divide and Conquer** (Разделяй и Властвуй). Весь процесс разбивается на три этапа:

### 1. Распределение данных (Distribution)
Главный процесс (Rank 0) инициализирует исходный массив. Поскольку размер массива $M$ может не делиться нацело на количество процессов $P$, используется алгоритм равномерного распределения с остатком:
*   Базовый размер блока: $M // P$.
*   Первые $M \% P$ процессов получают на 1 элемент больше.

Для передачи данных используется функция `MPI_Scatterv` (Vector Scatter). В отличие от обычного `Scatter`, она позволяет передавать блоки разного размера и требует предварительного расчета массивов `rcounts` (размеры блоков) и `displs` (смещения в памяти).

### 2. Локальные вычисления (Local Computation)
Каждый процесс, включая Rank 0, получает свой подмассив. Вычисление суммы происходит независимо и параллельно:
*   В **C++** используется `std::accumulate`.
*   В **Python** используется `sum` из стандартной библиотеки.

На этом этапе достигается основное ускорение, так как работа делится между ядрами процессора.

### 3. Каскадная сборка суммы (Manual Reduction)
Вместо последовательной отправки всех результатов на Rank 0 (что заняло бы $O(P)$ времени), используется **древовидная схема редукции** (Tree-based Reduction). Это позволяет собрать сумму за $O(\log_2 P)$ шагов.

**Логика цикла:**
Переменная `step` начинается с 1 и удваивается на каждой итерации (`1, 2, 4, ...`).
Условие `if (rank / step % 2 == 0)` определяет роль процесса:

*   **Получатель (Receiver):** Принимает частичную сумму от соседа (`rank + step`) и прибавляет её к своей. Продолжает участие в следующей итерации.
*   **Отправитель (Sender):** Отправляет свою накопленную сумму соседу (`rank - step`) и **выходит из цикла** (завершает работу).

#### Визуализация (на примере 8 процессов)

**Шаг 1 (step = 1):** Соседи обмениваются данными. Половина процессов отсеивается.
*   `0` ← `1` (0 суммирует, 1 завершает)
*   `2` ← `3`
*   `4` ← `5`
*   `6` ← `7`

**Шаг 2 (step = 2):** Работают только процессы 0, 2, 4, 6. Дистанция удваивается.
*   `0` ← `2` (0 суммирует результат пары 2+3, процесс 2 завершает)
*   `4` ← `6` (4 суммирует результат пары 6+7, процесс 6 завершает)

**Шаг 3 (step = 4):** Работают только процессы 0 и 4.
*   `0` ← `4` (0 получает сумму второй половины массива)

**Итог:** Процесс 0 содержит полную сумму всех элементов.


## Требования к запуску
**C++**: `OpenMPI`, `CMake 3.3+`.

**Python3**: `OpenMPI`, `mpi4py`, `numpy`.

## Запуск
Проект имеет два варианта запуска: автоматический и ручной.

### Автоматический запуск
Для автоматического запуска тестов на разном количестве процессов (1, 2, 4, 8) и вывода сравнительной таблицы используйте скрипт (требуется **Python**) в корневой папке.

Запуск бенчмарка:
```commandline
python benchmark.py
```

Пример вывода:
```
Proc  | Lang   | Sum Result           | Time (sec)      
--------------------------------------------------------
1     | C++    | xxx                  | yyy        
1     | Py     | xxx                  | yyy        
--------------------------------------------------------
2     | C++    | xxx                  | yyy        
2     | Py     | xxx                  | yyy        
--------------------------------------------------------
```

## Ручной запуск
Если вы хотите запустить конкретную реализацию отдельно.
1. **C++** Реализация
Находится в папке cpp.
Синтаксис:
```commandline
cd cpp
./run.sh <количество_процессов> [--debug]
```

При наличии флага `--debug` скомпилирует код с выводом логов (DLOG). В консоли будет видно, какой процесс кому отправляет данные.
 
2. **Python** Реализация
Находится в папке py.
Запуск:
```commandline
cd python
./run.sh <количество_процессов>
```